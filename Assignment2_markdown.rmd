---
title: "Assignment 2"
output:
    pdf_document: default
---

Intializing the libraries and setting the seed:
```{r}
set.seed(1006274274);
library(ggplot2)
library(MASS)
library(rcompanion)
library(lmtest)
```


Question 1:
```{r}
myTable <- read.table("vote.txt", header = TRUE, sep = "", dec = ".")

growthArr <-myTable$growth;
voteArr <- myTable$vote;


error =rnorm(length(growthArr),0,3.9);
fakeYData = vector()

for (i in 1:length(growthArr)){
  fakeYData = append(fakeYData, 46.3 + 4* growthArr[i] + error[i])
}

fakeDataModel = lm (fakeYData ~ growthArr)

summary(fakeDataModel)
df = data.frame(growthArr = 0.1)

predict(fakeDataModel,newdata = df)
#By default the prediction is 95%
predict(fakeDataModel, newdata = df, interval = "confidence")


```
As shown in the summary, the intercept is estimated to be at 44.77, and the slope is estimated to be at 5.26.

The predicted value of $E(Y | X_0 = 0.1)$ is 45.29.

The predicted interval of $[41.81 , 48.77]$ is the same as if we calculated it by hand, using the formula:
predicted-value $\pm$ t-value $\times$ error.


Question 1B)

```{r}
generateFakeDataModel <- function (xVector){

  error =rnorm(length(xVector),0,3.9);
  fakeYData = vector()

  for (i in 1:length(xVector)){
    fakeYData = append(fakeYData, 46.3 + 4* xVector[i] + error[i])
  }

  return ( lm (fakeYData ~ xVector))
}

interceptVector = vector()
slopeVector = vector()

for (i in 1:10000){
  linModel <- generateFakeDataModel(growthArr);
  interceptVector = append(interceptVector, coef(linModel)["(Intercept)"])
  slopeVector = append (slopeVector, coef(linModel)["xVector"])
}

plotNormalHistogram( unlist(interceptVector), prob = FALSE,
                     main = "Q1 part B: Intercept histogram",
                     length = 1000)

plotNormalHistogram( unlist(slopeVector), prob = FALSE,
                     main = "Q1 part B: Slope histogram",
                     length = 1000)


data_frame = data.frame(growthArr = growthArr, fakeYData = fakeYData)

cat ("The mean of the intercept vector is expected to be 46.3, and it is:", mean(interceptVector), "\n")
cat ("The mean of the slope vector is expected to be 4, and it is:", mean(slopeVector), "\n")

# Yes, the simulated results are consistent with theoretical results.
sigmaHat = 3.9

SXX <- sum((growthArr - mean(growthArr))^2)

#Standard error of slope (beta 1 hat), should be the same as the sd of the slope vector
stanErrorBeta1 <- sigmaHat / sqrt(SXX)

cat("The standard deviation of the slope is expected to be", sd(slopeVector),
    " and it is estimated to be", stanErrorBeta1, "\n")




#Standard error of intercept (beta 0 hat), should be the same as the sd of the slope intercept

stanErrorBeta0 <- sqrt(1 / length(growthArr) + (mean(growthArr)^2)/SXX) * sigmaHat

cat("The standard deviation of the intercept is expected to be", sd(interceptVector),
    " and it is estimated to be", stanErrorBeta0, "\n")
```

Yes, the results are absolutely consistent with the theoretical values,
as shown above.

Part C)

```{r}

df = data.frame(xVector = 0.1)

total = 10000
expectedVal = 46.3 + (4 * 0.1)

valCountInConfInt = 0
for (i in 1:total){


  currPrediction = predict(generateFakeDataModel(growthArr), newdata = df, interval = "confidence")
  if (currPrediction[[2]] <= expectedVal & expectedVal <= currPrediction[[3]]){
    valCountInConfInt = valCountInConfInt + 1
  }
}

cat ("Percentage of the expected val in the generated CIs:", valCountInConfInt / total, "\n")

# This is absolutely consistent with the predicted values.


```

Again, this is absolutely consistent with the predicted values


Question 2)
```{r}
x <- c(0:9)
y <- c(98, 135, 162, 178, 221, 232, 283, 300, 374, 395)
par(mar=c(1,1,1,1))
fit <- lm (y~x)
result <- boxcox(fit, lambda = seq(-2,2,0.1))

mylambda <- result$x[which.max(result$y)]

result = boxcox(y~x, lambda = seq(-2,2,0.1))
cat("The optimal lambda given by R is: ", result$x[which.max(result$y)], "\n")

valOf_n = length(x)

k2 = (prod(y))^(1/valOf_n)

optimal_lambda = -42
minSSE = .Machine$integer.max
for (lambda in seq(-2,2,0.1)){
  W = vector()

  for (i in 1:length(y)){

    if(lambda != 0){
      k1 = 1/(lambda * k2^(lambda-1) )
      w_i = k1 * ((y[i]^lambda) - 1)
    }else{
      w_i = k2 * (log(y[i]))
    }
    W = append(W,w_i)
  }

  tempModel = lm(W~x)
  cf <- coef(tempModel)
  intercept <- cf["(Intercept)"]
  slope <- cf["x"]
  # cat("The intercept is: ", intercept, " the slope is: ", slope, "\n")
  SSE = 0
  for(i in 1:length(x)){
    w_i_hat = intercept + slope * x[i]
    SSE = SSE + (W[i] - w_i_hat)^2
  }

  if(SSE < minSSE){
    minSSE = SSE
    optimal_lambda = lambda
  }

  W = vector()


}

cat("The optimal lambda that we've calculated is:", optimal_lambda, "\n")

```
This is pretty much the same result. I can get more precise values if I changed
the sequence to iterate by a smaller value. Currently, it's iterating by 0.1, and so
0.5 is the most accurate value it will generate.


Question 3)

Part a)
```{r}
kidiqTable = read.table("kidiq.csv",header = TRUE,sep = ",", dec = ".")

xAxis = kidiqTable$mom.iq
yAxis = kidiqTable$kid.score
modelForIQandScore = lm (yAxis ~ xAxis)


#Small p-value means reject null, so variance is constant

 predict(modelForIQandScore, newdata = data.frame(xAxis = 110), interval = "confidence")
# Mom.work is probably how much money mom makes. 4= the most, 1=the least
n = length(kidiqTable[[1]])

t_val = qt(p=0.025, df=432, lower.tail = FALSE)
# summary(modelForIQandScore)
sumOfSquareDev =   sum( (xAxis - mean(xAxis) )^2 )
MSE = mean(modelForIQandScore$residuals^2)
stanError = sqrt(MSE * (1/n + ((110 - mean(xAxis))^2 / sumOfSquareDev)))

upperIntVal = 92.79 + t_val * stanError
cat ("Upper interval is: ", upperIntVal, "\n")

lowerIntVal = 92.79 - t_val * stanError
cat ("Lower interval is: ", lowerIntVal, "\n")

# Both values match the R values generated by predict.
```
Again, the manually computed values (using R) are pretty much identical
to the ones generated by the predict() function. Only slight rounding errors cause the small differences.

Part B)

```{r}
t_val_partB = qt(p=0.995, df=432, lower.tail = TRUE)

stanError_partB = sqrt(MSE * (1 + 1/n + ((110-mean(xAxis))^2 / sumOfSquareDev)))

upperIntVal_partB = 92.79 + t_val_partB * stanError_partB
cat ("Upper interval is: ", upperIntVal_partB, "\n")

lowerIntVal_partB = 92.79 - t_val_partB * stanError_partB
cat ("Lower interval is: ", lowerIntVal_partB, "\n")


predict(modelForIQandScore, newdata = data.frame(xAxis = 110), interval = "confidence", level = 0.99)
```

Part c)
```{r}
plot(modelForIQandScore)

stanResiduals = rstandard(modelForIQandScore)


qqnorm(stanResiduals,
       ylab="Standardized Residuals", xlab="Normal Scores", main="QQ plot of residuals")
qqline(stanResiduals)
```

This looks pretty accurate to what we would expect.

Part d)
```{r}
shapiro.test(modelForIQandScore$residuals)

```
The p-value is 0.00217, which is < 0.05 (our alpha level), therefore we reject the null hypothesis that the data is normally distributed.

Part e)
```{r}
bptest(modelForIQandScore)
```

For the breusch-pagan test, we test the errors in the regression.
Null hypothesis is that all the error variances are the same
Alternate hypothesis is that they're not the same.
Since the p value is 0.013 < 0.05, we reject the null hypothesis.

Part f)
Since the errors are not normally distributed, nor have constant variance,
we should apply a boxcox transformation to make it more normal
```{r}
bc <- boxcox(yAxis ~ xAxis)

lambda = bc$x[which.max(bc$y)]

#fit new linear regression model using the Box-Cox transformation
new_model <- lm((yAxis^lambda) ~ xAxis)

print("The shapiro-wilk test on the boxcox transformed model")
shapiro.test(new_model$residuals)

bptest(new_model)

# The new p-value for the new_model is 0.3859,
# which is more than the alpha level,
# which means we fail to reject null hypothesis.

stanResidualsPartF = rstandard(new_model)

qqnorm(stanResidualsPartF,
       ylab="Standardized Residuals", xlab="Normal Scores", main="QQ plot of residuals for Part F")
qqline(stanResidualsPartF)
```

Now we've repeated the same tests, and we've failed to reject the null hypothesis in both times.
Therefore, now the errors are likely (not necessarily) to be normally distributed, as well as
likely having the same underlying variance.